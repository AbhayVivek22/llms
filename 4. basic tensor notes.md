# PyTorch Tensor Operations: Easy-to-Understand Notes

## 1. Tensor Basics & Creation

### `torch.tensor()`
```python
x = torch.tensor([1, 2, 3])
```
- Creates a tensor directly from data (lists, numpy arrays, etc.)
- Infers the data type from input or can be specified with `dtype`
- Like creating a container for your numbers that PyTorch can work with

### `torch.zeros()`, `torch.ones()`
```python
zeros = torch.zeros(2, 3)  # 2x3 tensor of zeros
ones = torch.ones(2, 3)    # 2x3 tensor of ones
```
- Creates tensors filled with 0s or 1s
- You specify the shape you want (rows, columns)
- Useful for initializing tensors or creating masks

### `torch.rand()`, `torch.randn()`
```python
random = torch.rand(2, 3)    # Uniform random [0,1]
normal = torch.randn(2, 3)   # Normal distribution (mean=0, std=1)
```
- `rand`: Random numbers between 0 and 1 (uniform distribution)
- `randn`: Random numbers from normal distribution (bell curve)
- Great for weight initialization in neural networks

### `torch.arange()`
```python
sequence = torch.arange(0, 10, step=2)  # tensor([0, 2, 4, 6, 8])
```
- Creates a sequence of numbers with specified start, end, and step
- Like Python's range but returns a tensor
- Useful for creating indices or sequence data

### Device Management
```python
x = torch.tensor([1, 2, 3], device="cuda:0")  # Create directly on GPU
# OR
x = torch.tensor([1, 2, 3]).to("cuda:0")      # Move existing tensor to GPU
```
- Controls whether tensors use CPU or GPU
- Moving to GPU (`cuda`) speeds up computations significantly
- Use `.to("cpu")` to move back if needed

## 2. Shape Manipulation

### `.reshape()`, `.view()`
```python
x = torch.arange(12)
reshaped = x.reshape(3, 4)  # Changes to 3x4 matrix
viewed = x.view(4, 3)       # Changes to 4x3 matrix
```
- Both change the shape without changing data
- **Key difference**: `view` only works if memory is contiguous; `reshape` always works
- Use `-1` for automatic dimension calculation: `x.reshape(2, -1)` → `(2, 6)`

### `.unsqueeze(dim)`
```python
x = torch.tensor([1, 2, 3])  # Shape: [3]
x_unsqueezed = x.unsqueeze(0)  # Shape: [1, 3] (adds dimension at position 0)
```
- Adds a dimension of size 1 at the specified position
- Crucial for adding batch or channel dimensions
- Think of it like "wrapping" your tensor in an extra layer

### `.squeeze(dim)`
```python
x = torch.zeros(1, 3, 1)  # Shape: [1, 3, 1]
x_squeezed = x.squeeze()  # Shape: [3] (removes all dimensions of size 1)
x_squeezed_dim = x.squeeze(0)  # Shape: [3, 1] (only removes dim 0)
```
- Removes dimensions of size 1
- Use with no arguments to remove all size-1 dimensions
- Specify dimension to remove only that one (if it's size 1)

### `.permute()`
```python
x = torch.rand(2, 3, 4)  # [batch, channels, length]
x_permuted = x.permute(1, 0, 2)  # [channels, batch, length]
```
- Rearranges dimensions in any order you specify
- Each argument is the new position of the original dimension
- Like reshuffling the axes of your tensor

### `.transpose(dim0, dim1)`
```python
x = torch.rand(2, 3)
x_transposed = x.transpose(0, 1)  # Swaps dimensions 0 and 1, shape becomes [3, 2]
```
- Swaps exactly two dimensions
- Simpler version of permute for when you only need to swap two axes
- Common for matrix transposition (rows ↔ columns)

### `.flatten()`
```python
x = torch.rand(2, 3, 4)
x_flat = x.flatten()  # Shape: [24]
x_flat_dim = x.flatten(1)  # Shape: [2, 12] (flattens from dim 1 onward)
```
- Collapses tensor into 1D (or partially flattens from specified dimension)
- Like squishing all the values into a single long line
- Useful before feeding data into fully connected layers

## 3. Indexing & Slicing

### Basic Indexing
```python
x = torch.tensor([[1, 2, 3], [4, 5, 6]])
element = x[0, 1]  # Value: 2 (row 0, column 1)
row = x[0]  # Values: [1, 2, 3] (entire row 0)
column = x[:, 1]  # Values: [2, 5] (entire column 1)
```
- Works like NumPy/Python indexing
- Commas separate dimensions
- Colon (`:`) means "all indices in this dimension"

### Slicing
```python
x = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])
subset = x[0:2, 1:3]  # Values: [[2, 3], [6, 7]]
stride = x[:, ::2]  # Values: [[1, 3], [5, 7], [9, 11]] (every 2nd column)
```
- Format: `start:end:step`
- Extracts ranges of elements
- Step can skip elements (like `::2` for every second element)

### Boolean Indexing
```python
x = torch.tensor([1, -1, 2, -3, 5])
mask = x > 0  # tensor([True, False, True, False, True])
positive = x[mask]  # Values: [1, 2, 5]
```
- Create a boolean mask and use it for indexing
- Returns only elements where mask is `True`
- Great for filtering data based on conditions

### Fancy Indexing
```python
x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
indices = torch.tensor([0, 2])
selected_rows = x[indices]  # Values: [[1, 2, 3], [7, 8, 9]]
```
- Use tensor of indices to select multiple elements
- Allows non-sequential selection
- Useful for gathering embeddings or implementing attention

## 4. Broadcasting

### Concept
Broadcasting automatically expands tensors of different shapes to make operations work.

```python
# Adding scalar to tensor
x = torch.tensor([[1, 2], [3, 4]])
y = x + 10  # Adds 10 to every element

# Adding vector to matrix
row_vector = torch.tensor([10, 20])
z = x + row_vector  # Adds [10,20] to each row
# Result: [[11, 22], [13, 24]]
```

### Broadcasting Rules
1. Dimensions are aligned from the right
2. Size-1 dimensions are stretched to match larger dimensions
3. Missing dimensions are treated as size 1

### Manual Broadcasting with `.unsqueeze()`
```python
# Broadcasting a vector across batch dimension
batch = torch.rand(32, 10)  # [batch_size, features]
scale = torch.rand(10)  # [features]
scaled = batch * scale.unsqueeze(0)  # [1, features] broadcasts across batch
```
- Sometimes you need to add dimensions explicitly for clarity
- Particularly important in complex operations

## 5. Tensor Math & Reduction Ops

### Element-wise Operations
```python
a = torch.tensor([1, 2, 3])
b = torch.tensor([4, 5, 6])

add = a + b  # [5, 7, 9]
subtract = a - b  # [-3, -3, -3]
multiply = a * b  # [4, 10, 18]
divide = a / b  # [0.25, 0.40, 0.50]
power = a ** 2  # [1, 4, 9]
```
- Apply the operation to each pair of elements
- Tensors must be the same shape (or broadcastable)
- Fast and parallelizable on GPUs

### Reduction Operations
```python
x = torch.tensor([[1, 2, 3], [4, 5, 6]])

total_sum = torch.sum(x)  # 21 (sum of all elements)
row_sums = torch.sum(x, dim=1)  # [6, 15] (sum of each row)
col_sums = torch.sum(x, dim=0)  # [5, 7, 9] (sum of each column)
```
- Collapse tensor along specified dimensions
- `dim` parameter controls which dimension to reduce
- `keepdim=True` preserves reduced dimensions as size 1

### Common Reductions
```python
mean = torch.mean(x, dim=0)  # Mean along columns
max_vals, max_indices = torch.max(x, dim=1)  # Maximum value and index in each row
min_vals = torch.min(x, dim=0).values  # Minimum in each column
std = torch.std(x, dim=1)  # Standard deviation of each row
```
- Often used for aggregating features or calculating statistics
- The `.values` attribute extracts just the values (not indices)
- Essential for things like pooling layers in CNNs

### Matrix Multiplication
```python
a = torch.rand(2, 3)
b = torch.rand(3, 4)

# Three equivalent ways:
c1 = torch.matmul(a, b)  # Shape: [2, 4]
c2 = a @ b  # Same as matmul (recommended)
c3 = torch.mm(a, b)  # Legacy method
```
- Matrix multiplication, not element-wise
- Last dimension of first tensor must match second-to-last of second tensor
- Essential for neural network layers

## 6. expand() vs repeat() vs repeat_interleave()

### `expand()`
```python
x = torch.tensor([[1], [2]])  # Shape: [2, 1]
expanded = x.expand(2, 3)  # Shape: [2, 3], values: [[1, 1, 1], [2, 2, 2]]
```
- Creates a view without copying data
- Only size-1 dimensions can be expanded
- Memory-efficient when you don't need to modify the expanded values
- Like stretching a tensor without creating new memory

### `repeat()`
```python
x = torch.tensor([[1, 2], [3, 4]])  # Shape: [2, 2]
repeated = x.repeat(2, 3)  # Shape: [4, 6], repeats entire tensor in a tiled pattern
```
- Actually copies the data, creating new memory
- Repeats the entire tensor in each dimension
- Arguments specify how many times to repeat in each dimension
- Think of it as tiling copies of the original tensor

### `repeat_interleave()`
```python
x = torch.tensor([1, 2, 3])
interleaved = x.repeat_interleave(2)  # [1, 1, 2, 2, 3, 3]
```
- Repeats each element individually
- Can specify different repeat counts for each element
- Useful for expanding sequence lengths or duplicating specific elements
- Think of it as repeating individual elements rather than the whole tensor

## 7. Batch Operations

### Normalization
```python
# L2 normalization of vectors in a batch
batch = torch.rand(32, 10)  # [batch_size, features]
norm = torch.norm(batch, p=2, dim=1, keepdim=True)  # Calculate L2 norm along feature dimension
normalized = batch / norm  # Divide each vector by its norm
```
- Process multiple examples simultaneously
- Operate across the batch dimension for efficiency
- Common in preprocessing and model layers

### Pairwise Distances
```python
# Compute distances between two sets of vectors
a = torch.rand(5, 3)  # 5 vectors
b = torch.rand(4, 3)  # 4 vectors
a_expanded = a.unsqueeze(1)  # Shape: [5, 1, 3]
b_expanded = b.unsqueeze(0)  # Shape: [1, 4, 3]
dist = torch.sqrt(torch.sum((a_expanded - b_expanded)**2, dim=2))  # Shape: [5, 4]
```
- Calculates distance between every pair of vectors
- Uses broadcasting to avoid loops
- Useful for similarity metrics and attention mechanisms

### Masking
```python
# Apply mask to attention scores
scores = torch.rand(8, 4, 10, 10)  # [batch, heads, seq_len, seq_len]
mask = torch.ones(10, 10).triu(diagonal=1).bool()  # Upper triangular mask
scores.masked_fill_(mask, -1e9)  # Fill masked positions with large negative value
```
- Selectively modify or hide parts of tensors
- Important for sequence models to prevent attending to future tokens
- Often combined with broadcasting to apply across batches

## 8. Real Model-Like Use Cases

### Multi-Head Attention Reshape
```python
# Split input for multi-head attention
batch_size, seq_len, hidden_dim = 32, 20, 512
num_heads = 8
head_dim = hidden_dim // num_heads

x = torch.rand(batch_size, seq_len, hidden_dim)
# Reshape to separate heads
reshaped = x.view(batch_size, seq_len, num_heads, head_dim)
# Permute to get batch and heads as first dimensions
permuted = reshaped.permute(0, 2, 1, 3)  # [batch, heads, seq_len, head_dim]
```
- Transforms a single representation into multiple attention heads
- Enables attention to focus on different aspects of the input
- Core operation in transformer models

### Positional Encoding
```python
# Create sinusoidal positional encodings
seq_len = 50
d_model = 512
positions = torch.arange(seq_len).unsqueeze(1)  # [seq_len, 1]
div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))
pos_encoding = torch.zeros(seq_len, d_model)
pos_encoding[:, 0::2] = torch.sin(positions * div_term)
pos_encoding[:, 1::2] = torch.cos(positions * div_term)
```
- Creates unique encodings for each position in a sequence
- Helps models understand the order of elements
- Essential for transformer models that process all tokens simultaneously

### One-Hot Encoding
```python
# Convert class indices to one-hot vectors
indices = torch.tensor([0, 3, 1])  # Batch of 3 class indices
num_classes = 4
one_hot = torch.zeros(3, num_classes)
one_hot.scatter_(1, indices.unsqueeze(1), 1)  # Result: [[1,0,0,0], [0,0,0,1], [0,1,0,0]]
```
- Represents categorical data as binary vectors
- Each position corresponds to a class
- `.scatter_()` efficiently fills the correct positions with 1s
- Used for classification tasks and embedding lookups

---

These operations form the foundation of tensor manipulation in PyTorch. Understanding them will make it much easier to implement and debug deep learning models. The best way to learn is to experiment with these operations in small code snippets before incorporating them into larger models.