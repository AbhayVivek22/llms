{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0969a969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aff0aae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 3D tensor of shape (2, 3, 4) filled with random values from a normal distribution.\n",
    "a = torch.rand(2,3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8217f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform element-wise multiplication between two tensors of shapes (3, 4) and (4) (broadcasting).\n",
    "\n",
    "a = torch.rand(3,4) # this has shape [3,4]\n",
    "b = torch.rand(4)   # this has shape [4]\n",
    "\n",
    "# if we just do a * b , then b will be broadcasted to a\n",
    "c = a * b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03f28521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of b -  torch.Size([2, 12])\n",
      "shape of c -  torch.Size([6, 4])\n"
     ]
    }
   ],
   "source": [
    "# Reshape a tensor of shape (2, 3, 4) to (2, 12) and then to (6, 4).\n",
    "a = torch.rand(2,3,4)\n",
    "\n",
    "b = a.view(2,-1)\n",
    "print(\"shape of b - \", b.shape)\n",
    "\n",
    "c = b.view(6,-1)\n",
    "print(\"shape of c - \", c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24113f07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 4])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Implement a function that performs batch matrix multiplication between tensors of shapes \n",
    "# (batch_size, seq_len, d_model) and (batch_size, d_model, seq_len).\n",
    "batch_size = 3\n",
    "seq_len = 4\n",
    "d_model = 8\n",
    "\n",
    "a = torch.rand(batch_size, seq_len, d_model)\n",
    "b = torch.rand(batch_size, d_model, seq_len)\n",
    "# NOTE - a batch multiplication bw above two means - that for every word in the sequence , \n",
    "# we get similarity score with every other word\n",
    "\n",
    "c = torch.bmm(a,b)\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c79a36c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1., 1., 1.],\n",
       "        [0., 0., 1., 1., 1.],\n",
       "        [0., 0., 0., 1., 1.],\n",
       "        [0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a masking function that generates a look-ahead mask for a sequence of length n \n",
    "# (upper triangular matrix with zeros on and below diagonal, ones above).\n",
    "ones = torch.ones(5,5)\n",
    "a = torch.triu(ones, diagonal=1)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89479e88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0000e+09,  7.2933e-01,  1.2759e-01,  3.1026e-01,  9.0654e-02],\n",
       "        [-1.0000e+09, -1.0000e+09,  6.4935e-01,  9.2305e-01,  8.3734e-01],\n",
       "        [-1.0000e+09, -1.0000e+09, -1.0000e+09,  1.4614e-02,  9.3650e-01],\n",
       "        [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,  5.9154e-01],\n",
       "        [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Implement a function that applies a mask to attention logits, \n",
    "# replacing masked values with a large negative number.\n",
    "\n",
    "b = torch.rand(5,5)\n",
    "c = b.masked_fill_(a==0, value=-1e9)\n",
    "# note -- above i did a==0 , it is giving a boolean to the masked_fill_ to just know where to put the value\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19856b70",
   "metadata": {},
   "source": [
    "##### ‚úÖ Even if the sequence length is different, the embedding size stays the same (e.g., 512).\n",
    " But still ‚Äî\n",
    " ‚ùì Why do we need padding at all?\n",
    "\n",
    " In Transformers, we batch sequences together to compute attention efficiently using matrix operations.\n",
    " But matrix operations (like dot product across time steps) require all sequences in \n",
    " the batch to have the same sequence length (same number of time steps).\n",
    "\n",
    "\n",
    "```\n",
    "Padded Input (after embeddings, assuming d_model=4):\n",
    "\n",
    "Sentence 0: [e1, e2, e3, PAD]   ‚Üí shape: (4, d_model)\n",
    "Sentence 1: [e1, e2, e3, e4]    ‚Üí shape: (4, d_model)\n",
    "```\n",
    "\n",
    "\n",
    "| Term                       | Description                                                                                                             |\n",
    "| -------------------------- | ----------------------------------------------------------------------------------------------------------------------- |\n",
    "| `embedding size (d_model)` | The **dimensionality of word representations**. Fixed size (e.g., 512).                                                 |\n",
    "| `sequence length`          | The **number of words/tokens** in a sentence. Varies (e.g., 6 words, 10 words...)                                       |\n",
    "| `padding`                  | Dummy tokens (usually all zeros) added to shorter sequences to make their length equal to the longest one in the batch. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0038ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch input shape: torch.Size([2, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "# Implement a function that efficiently computes attention scores for sequences of \n",
    "# variable length within a batch.\n",
    "\n",
    "\n",
    "# ‚úÖ Step 1: Understand the scenario\n",
    "# Say we have a batch of 2 sequences:\n",
    "# Sequence A: \"I love transformers\" ‚Üí length 3\n",
    "# Sequence B: \"Deep learning is awesome\" ‚Üí length 4\n",
    "# To batch them together in PyTorch, we must pad them to the same length (max = 4).\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Simulate embedded sequences (e.g., from an embedding layer)\n",
    "# We'll use random vectors to represent tokens\n",
    "\n",
    "d_k = 4  # embedding size\n",
    "\n",
    "# Sequence A: 3 tokens + 1 padding\n",
    "seq1 = torch.rand(3, d_k)\n",
    "padded_seq1 = F.pad(seq1, pad=(0, 0, 0, 1))  # pad 1 row on bottom\n",
    "\n",
    "# Sequence B: 4 tokens (no padding needed)\n",
    "padded_seq2 = torch.rand(4, d_k)\n",
    "\n",
    "# Stack into a batch\n",
    "x = torch.stack([padded_seq1, padded_seq2])  # Shape: (2, 4, 4)\n",
    "print(\"Batch input shape:\", x.shape)  # (batch_size, seq_len, d_k)\n",
    "\n",
    "# Create padding mask: 1 for real tokens, 0 for padding\n",
    "mask = torch.tensor([\n",
    "    [1, 1, 1, 0],  # 1st sequence has padding in last position\n",
    "    [1, 1, 1, 1],  # 2nd sequence has all real tokens\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "def masked_attention(q, k, v, mask):\n",
    "    # dot product to get attention scores\n",
    "    scores = torch.bmm(q, k.transpose(-1,-2))\n",
    "\n",
    "    # step 2 - scale \n",
    "    d_k = q.size(-1)\n",
    "    scores = scores / torch.sqrt(torch.tensor(d_k))\n",
    "\n",
    "    # step 3 - expand mask and apply \n",
    "    # currently mask shape is 2,4 - we need it in 3d\n",
    "    mask = mask.unsqueeze(1)\n",
    "    scores = scores.masked_fill_(mask==0, float(-1e9))\n",
    "\n",
    "    atten_weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "    output = torch.bmm(atten_weights, v)\n",
    "\n",
    "    return output, atten_weights\n",
    "\n",
    "\n",
    "# how it works\n",
    "\n",
    "\n",
    "# [ I   love   transformers   <PAD> ]   ‚Üí Mask = [1, 1, 1, 0]\n",
    "# [ Deep learning is awesome     ]     ‚Üí Mask = [1, 1, 1, 1]\n",
    "\n",
    "# Attention scores:\n",
    "# ‚úì Real tokens attend to each other.\n",
    "# ‚úó PAD tokens are ignored (masked with -inf ‚Üí softmax ‚Üí 0).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7633ecb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function that applies padding masks and causal masks \n",
    "# simultaneously for transformer decoder training.\n",
    "\n",
    "\n",
    "# üß† What are the two masks?\n",
    "#   Padding mask:\n",
    "#       Masks out the PAD tokens in the input.\n",
    "#       Shape: (batch_size, seq_len) ‚Üí 1 for real token, 0 for pad.\n",
    "#       Usually comes from input data.\n",
    "#   Causal mask (look-ahead mask):\n",
    "#       Ensures that at position t, the model can‚Äôt see future tokens (t+1, t+2, ...).\n",
    "#       Shape: (seq_len, seq_len) ‚Üí Upper triangular matrix with -inf above the diagonal.\n",
    "\n",
    "causal_mask = torch.tril(torch.ones(seq_len, seq_len), diagonal=0)\n",
    "\n",
    "def attention_layer(q,k,v,pad_mask, causal_mask):\n",
    "    # finding attention scores\n",
    "    scores = torch.bmm(q, k.transpose(-1,-2))\n",
    "\n",
    "    # scaling the scores\n",
    "    scores = scores / torch.sqrt(torch.tensor(k.size(-1), dtype=torch.float32))\n",
    "\n",
    "    # first we make their dimensions compatible\n",
    "    pad_mask = pad_mask.unsqueeze(1)\n",
    "    causal_mask = causal_mask.unsqueeze(0).bool()\n",
    "    # Final mask: only allow attending to real tokens in the past\n",
    "    final_mask = pad_mask & causal_mask\n",
    "    # calculating the final combined mask \n",
    "    masked_scores = scores.masked_fill(final_mask==0, -1e9)\n",
    "\n",
    "    attn_wts = torch.softmax(masked_scores, dim=-1)\n",
    "    output = torch.bmm(attn_wts, v)\n",
    "\n",
    "    return output, attn_wts "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626b6600",
   "metadata": {},
   "source": [
    "# Exercise Set 2: Scaled Dot-Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea274c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement scaled dot-product attention using PyTorch operations.\n",
    "\n",
    "# note - we will be doing masked version\n",
    "def scaled_dot_prod_attn(q, k, v, mask):\n",
    "    scores = torch.bmm(q, k.transpose(-1, -2))\n",
    "\n",
    "    # scaled scores\n",
    "    scores = scores / torch.sqrt(torch.tensor(k.size(-1), dtype=torch.float32))\n",
    "\n",
    "    masked_score = scores.masked_fill(mask==0, -1e9)\n",
    "\n",
    "    attention_wts = torch.softmax(masked_score, dim=-1)\n",
    "\n",
    "    # (Optional) Avoid NaNs from softmax over -inf only\n",
    "    attention_wts = attention_wts.masked_fill(torch.isnan(attention_wts), 0)\n",
    "\n",
    "    output = torch.bmm(attention_wts, v)\n",
    "\n",
    "    return output, attention_wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4e0768d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final mask size -  torch.Size([2, 4, 4])\n",
      "scores shape -  torch.Size([2, 4, 4])\n",
      "output shape -  torch.Size([2, 4, 8])\n"
     ]
    }
   ],
   "source": [
    "# Implement a batched version of scaled dot-product attention that handles \n",
    "# multiple examples simultaneously.\n",
    "# Add support for attention masking to handle variable-length sequences.\n",
    "\n",
    "batch_size = 2\n",
    "seq_len = 4\n",
    "d_k = 8\n",
    "\n",
    "# Dummy queries, keys, values\n",
    "q = torch.randn(batch_size, seq_len, d_k)\n",
    "k = torch.randn(batch_size, seq_len, d_k)\n",
    "v = torch.randn(batch_size, seq_len, d_k)\n",
    "\n",
    "# Padding mask (for example: first sequence has length 3, second has length 2)\n",
    "pad_mask = torch.tensor([\n",
    "    [1,1,1,0],\n",
    "    [1,1,0,0]\n",
    "])\n",
    "# its in the shape -- B, S - 2,4\n",
    "\n",
    "pm = pad_mask.unsqueeze(1) # now its shape - 2,1,4\n",
    "# note -- WE HAVE TO MASK THE ATTENTION SCORES SO THE MASK HAS TO BE IN SHAPE -- 2,4,4\n",
    "\n",
    "# NOW IF WE DO \n",
    "final_mask = pad_mask.unsqueeze(1) & torch.ones(batch_size, seq_len, seq_len, dtype=bool) # -- one gives shape 2,1,4 and one gives shape 2,4,1\n",
    "print(\"final mask size - \", final_mask.shape)\n",
    "\n",
    "\n",
    "# Compute scores\n",
    "scores = torch.bmm(q, k.transpose(-1, -2))  # (B, S, S)\n",
    "scores = scores / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "print(\"scores shape - \", scores.shape)\n",
    "# Masking\n",
    "scores = scores.masked_fill(final_mask == 0, -1e9)\n",
    "\n",
    "\n",
    "# Softmax\n",
    "attn_weights = torch.softmax(scores, dim=-1)\n",
    "\n",
    "# Attention output\n",
    "output = torch.bmm(attn_weights, v)  # (B, S, D)\n",
    "\n",
    "print(\"output shape - \", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e60db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a memory-efficient version of attention that processes long sequences in chunks.\n",
    "\n",
    "\n",
    "# ‚úÖ Part 1: Chunked (Windowed) Attention\n",
    "# Instead of computing attention over the entire sequence, \n",
    "# we divide the sequence into non-overlapping or overlapping chunks, \n",
    "# and apply attention within each chunk.\n",
    "\n",
    "#   üîç Why?\n",
    "#       Reduces memory from O(N¬≤) to O(N * W) where W is chunk/window size.\n",
    "#       Example: instead of attending to 10,000 tokens at once, attend to 512 tokens at a time.\n",
    "\n",
    "\n",
    "\n",
    "def chunked_attention(q, k, v, chunk_size):\n",
    "    B, S, D = q.size()\n",
    "\n",
    "    chunks = S // chunk_size\n",
    "\n",
    "    output = []\n",
    "\n",
    "\n",
    "    for i in range(chunks):\n",
    "        start = i * chunk_size\n",
    "        end = start + chunk_size\n",
    "\n",
    "        # now we can easily take out the batches from the given sequences\n",
    "        q_chunk = q[:, start:end, :]\n",
    "        k_chunk = k[:, start:end, :]\n",
    "        v_chunk = v[:, start:end, :]\n",
    "\n",
    "        scores = torch.bmm(q_chunk, k_chunk.transpose(-1,-2)) / torch.sqrt(torch.tensor(k_chunk.size(-1), dtype=torch.float32))\n",
    "        attention = torch.softmax(scores)\n",
    "        output_chunk = torch.bmm(attention, v_chunk)\n",
    "\n",
    "        output.append(output_chunk)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fed74a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement sparse attention patterns (e.g., local attention) that only attend to nearby positions.\n",
    "\n",
    "# Only allow each token to attend to its W neighbors, e.g., a window of ¬±2 tokens.\n",
    "\n",
    "def local_attention_mask(seq_len, window_size):\n",
    "    \"\"\"\n",
    "    Returns a (seq_len, seq_len) boolean mask for local attention.\n",
    "    1 means allowed, 0 means masked out.\n",
    "    \"\"\"\n",
    "    mask = torch.zeros(seq_len, seq_len, dtype=torch.bool)\n",
    "    for i in range(seq_len):\n",
    "        start = max(0, i - window_size)\n",
    "        end = min(seq_len, i + window_size + 1)\n",
    "        mask[i, start:end] = 1\n",
    "    return mask  # shape: (S, S)\n",
    "\n",
    "# üîß Code: Local Attention with Mask\n",
    "\n",
    "\n",
    "def local_attention(q, k, v, window_size):\n",
    "    \"\"\"\n",
    "    Applies sparse local attention: each token attends only to nearby positions.\n",
    "    \"\"\"\n",
    "    B, S, D = q.size()\n",
    "    mask = local_attention_mask(S, window_size).to(q.device)  # (S, S)\n",
    "    mask = mask.unsqueeze(0).expand(B, -1, -1)  # (B, S, S)\n",
    "\n",
    "    # Scaled dot-product attention\n",
    "    scores = torch.bmm(q, k.transpose(1, 2)) / torch.sqrt(torch.tensor(D, dtype=torch.float32))\n",
    "    scores = scores.masked_fill(~mask, -1e9)  # Apply local mask\n",
    "    attn = torch.softmax(scores, dim=-1)\n",
    "    output = torch.bmm(attn, v)  # (B, S, D)\n",
    "\n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8364d537",
   "metadata": {},
   "source": [
    "# Implement a basic multi-head attention module with 8 heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab9f5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "# Implement a basic multi-head attention module with 8 heads.\n",
    "b = 4\n",
    "s = 5\n",
    "d = 512\n",
    "heads = 8\n",
    "\n",
    "head_dim = d//heads\n",
    "print(head_dim)\n",
    "\n",
    "# what it has to do is create 3 vectors q,k,v from the given b,s,d and project them acc to the num of heads and their dimensions\n",
    "# so the 512 will be projected into the heads , head dimensions\n",
    "\n",
    "x = torch.rand(b, s, d) # this can be thought of as the input\n",
    "\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.module):\n",
    "    def __init__(self, d_model = d, num_heads = heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dimensions = d_model // num_heads\n",
    "\n",
    "        # linear layers to make q,k,v from the input\n",
    "        self.q = nn.Linear(d_model, d_model)\n",
    "        self.k = nn.Linear(d_model, d_model)\n",
    "        self.v = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, q, k, v, mask=None):\n",
    "        scores = torch.bmm(q, k.transpose(-1,-2))\n",
    "\n",
    "        scores = scores / torch.sqrt(torch.tensor(q.size(-1), dtype=torch.float32))\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask==0, -1e9)\n",
    "        \n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "        output = torch.bmm(attention_weights, v)\n",
    "        return output, attention_weights\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        B, S, D = x.size()\n",
    "\n",
    "        q = self.q(x)\n",
    "        k = self.k(x)\n",
    "        v = self.v(v)\n",
    "\n",
    "        # VERY IMPORTANT STEP -- SPLIT INTO HEADS \n",
    "        def reshape_to_heads(x):\n",
    "            return x.view(B, S, self.num_heads, self.head_dimensions).transpose(1,2)\n",
    "            # it reshapes B,S,D ----> B,S,H,H_dim -----> B, H, S, H_dim\n",
    "        \n",
    "        q = reshape_to_heads(q)\n",
    "        k = reshape_to_heads(k)\n",
    "        v = reshape_to_heads(v)\n",
    "\n",
    "        attn_output, attn_weights = self.scaled_dot_prod_attn(q , k , v, mask)\n",
    "\n",
    "        # concat all the heads \n",
    "        # here we do ----   [B, h, S, head_dim] -> [B, S, h * head_dim] \n",
    "        concat = attn_output.transpose(1,2).contiguous().view(B, S, self.d_model)\n",
    "\n",
    "\n",
    "        output = self.out_proj(concat)\n",
    "\n",
    "        return output, attn_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02959a4b",
   "metadata": {},
   "source": [
    "### Whats happening above \n",
    "\n",
    "\n",
    "## ‚úÖ How Scaled Dot-Product Attention works on `[B, h, S, head_dim]`\n",
    "\n",
    "### Step-by-step breakdown:\n",
    "\n",
    "Assume:\n",
    "\n",
    "* `B = batch size`\n",
    "* `h = number of heads`\n",
    "* `S = sequence length`\n",
    "* `head_dim = d_model / num_heads`\n",
    "\n",
    "Now, `Q`, `K`, and `V` all have shape `[B, h, S, head_dim]`.\n",
    "\n",
    "```python\n",
    "scores = torch.matmul(q, k.transpose(-2, -1))\n",
    "```\n",
    "\n",
    "### ü§î What‚Äôs happening here?\n",
    "\n",
    "* `q` has shape `[B, h, S, head_dim]`\n",
    "* `k.transpose(-2, -1)` flips the last two dims ‚Üí `[B, h, head_dim, S]`\n",
    "\n",
    "So this becomes:\n",
    "\n",
    "```\n",
    "[B, h, S, head_dim] @ [B, h, head_dim, S] ‚Üí [B, h, S, S]\n",
    "```\n",
    "\n",
    "### üéØ Meaning:\n",
    "\n",
    "You‚Äôre computing dot products between each query and all keys **across the sequence** for every **head** in every **batch**. The result is a matrix of attention **scores** showing how much each word attends to every other word.\n",
    "\n",
    "---\n",
    "\n",
    "## ü§Ø Visual example:\n",
    "\n",
    "Say you have 1 head (h=1), 1 query for a sentence of 5 words (S=5), each with `head_dim=3`:\n",
    "\n",
    "```\n",
    "q = [word1_q, word2_q, word3_q, word4_q, word5_q] ‚Üí shape [1, 1, 5, 3]\n",
    "k = [word1_k, ..., word5_k] ‚Üí shape [1, 1, 5, 3]\n",
    "```\n",
    "\n",
    "* You take dot products between each query vector and all key vectors: ‚Üí attention map `[5, 5]` per head\n",
    "* This is done **in parallel for all heads** using `torch.matmul`\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ What is `.contiguous()`?\n",
    "\n",
    "PyTorch stores tensors in memory in a way that can be **non-contiguous** after certain operations like `transpose`, `view`, or `permute`.\n",
    "\n",
    "### üëá For example:\n",
    "\n",
    "```python\n",
    "x = torch.randn(2, 3, 4)\n",
    "x = x.transpose(1, 2)  # x now has shape (2, 4, 3), but it's not contiguous in memory\n",
    "```\n",
    "\n",
    "If you now try to `.view()` it (reshape), PyTorch may throw an error because it expects contiguous memory layout.\n",
    "\n",
    "### üí° Solution:\n",
    "\n",
    "Use `.contiguous()` before `.view()`:\n",
    "\n",
    "```python\n",
    "x = x.contiguous().view(2, -1)\n",
    "```\n",
    "\n",
    "It tells PyTorch:\n",
    "\n",
    "> ‚ÄúPlease copy this tensor‚Äôs data into a fresh, contiguous memory layout so I can safely reshape it.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Summary:\n",
    "\n",
    "| Concept               | Explanation                                                              |\n",
    "| --------------------- | ------------------------------------------------------------------------ |\n",
    "| `[B, h, S, head_dim]` | Processes multi-head attention **in parallel** for all heads and batches |\n",
    "| `matmul(q, k.T)`      | Computes attention scores for every position against every other         |\n",
    "| `.contiguous()`       | Fixes memory layout so `.view()` or other reshapes work safely           |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de03f30",
   "metadata": {},
   "source": [
    "## üß† Implement a Multi-Head Attention Module with Different Dimensions for Keys, Queries, and Values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c82d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß† Implement a Multi-Head Attention Module with Different Dimensions for Keys, Queries, and Values\n",
    "\n",
    "# üéØ Goal:\n",
    "# Build a Multi-Head Attention mechanism where:\n",
    "#   query, key, and value have different input dimensions\n",
    "# For example:\n",
    "#   query comes from one source (e.g., decoder)\n",
    "#   key/value come from another (e.g., encoder)\n",
    "# This is especially needed in cross-attention.\n",
    "\n",
    "\n",
    "\n",
    "# Normally, all 3 have the same dimension d_model, and we split into n_heads:\n",
    "# q, k, v = x @ W_q, x @ W_k, x @ W_v  # Shape: (B, S, d_model)\n",
    "\n",
    "# But what if:\n",
    "# q = decoder_output  ‚Üí shape: [B, T_q, d_q]\n",
    "# k = encoder_output  ‚Üí shape: [B, T_kv, d_k]\n",
    "# v = encoder_output  ‚Üí shape: [B, T_kv, d_v]\n",
    "\n",
    "\n",
    "# You must:\n",
    "#     Use separate projection layers for q, k, v (with their own input dims)\n",
    "#     Still produce same head_dim to allow scaled dot-product attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c01952d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_q, d_k, d_v, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.head_dim = d_model // n_heads\n",
    "        assert self.head_dim * n_heads == d_model, \"d_model must be divisible by n_heads\"\n",
    "\n",
    "        # Separate linear layers for q, k, v with their input dims\n",
    "        self.q_proj = nn.Linear(d_q, d_model)\n",
    "        self.k_proj = nn.Linear(d_k, d_model)\n",
    "        self.v_proj = nn.Linear(d_k, d_model)\n",
    "\n",
    "        # Output projection\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69df7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß† 2. Split into heads\n",
    "def split_heads(self, x):\n",
    "    # x: [B, T, d_model] ‚Üí [B, n_heads, T, head_dim]\n",
    "    B, T, _ = x.size()\n",
    "    x = x.view(B, T, self.n_heads, self.head_dim)\n",
    "    return x.transpose(1, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de91cb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, q, k, v, mask=None):\n",
    "    # Project to common d_model\n",
    "    q = self.q_proj(q)  # [B, T_q, d_model]\n",
    "    k = self.k_proj(k)  # [B, T_kv, d_model]\n",
    "    v = self.v_proj(v)  # [B, T_kv, d_model]\n",
    "\n",
    "    # Split into heads\n",
    "    q = self.split_heads(q)  # [B, h, T_q, head_dim]\n",
    "    k = self.split_heads(k)  # [B, h, T_kv, head_dim]\n",
    "    v = self.split_heads(v)  # [B, h, T_kv, head_dim]\n",
    "\n",
    "    # Scaled dot-product attention\n",
    "    scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)  # [B, h, T_q, T_kv]\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "    attn = F.softmax(scores, dim=-1)\n",
    "\n",
    "    output = torch.matmul(attn, v)  # [B, h, T_q, head_dim]\n",
    "\n",
    "    # Concatenate heads\n",
    "    output = output.transpose(1, 2).contiguous()  # [B, T_q, h, head_dim]\n",
    "    output = output.view(output.size(0), output.size(1), self.d_model)  # [B, T_q, d_model]\n",
    "\n",
    "    return self.out_proj(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66883fbf",
   "metadata": {},
   "source": [
    "## Positional encodings\n",
    "\n",
    "‚úÖ Implement sinusoidal positional encodings and visualize them.\n",
    "\n",
    "üß† Concept Recap (Super Simple)\n",
    "Positional encoding gives each position in a sequence a unique vector based on sinusoids.\n",
    "\n",
    "    - Each dimension of the vector has a different wavelength.\n",
    "\n",
    "    - Even dims ‚Üí sin(pos / (10000^(2i/d_model)))\n",
    "\n",
    "    - Odd dims ‚Üí cos(pos / (10000^(2i/d_model)))\n",
    "    \n",
    "This gives a smooth, periodic, and unique encoding per position, and importantly ‚Äî it generalizes to longer sequences!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d051fb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_encodings(seq_len, d_model):\n",
    "    pe = torch.arange(seq_len).unsqueeze(1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1fe597",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f149727a",
   "metadata": {},
   "source": [
    "# Layer norm and Residual connections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7c98aa",
   "metadata": {},
   "source": [
    "Great ‚Äî here's a focused, step-by-step learning path that will make you master both **Layer Normalization** and **Residual Connections** without doing all those exercises individually. I‚Äôll group the essential ideas, code, and visual mental models together so you get everything in fewer steps.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Layer Normalization (LN) ‚Äî Simplified Mastery Path\n",
    "\n",
    "### üîπ What is it?\n",
    "\n",
    "It stabilizes training by normalizing each **input (per data point)** across its **features**, not across the batch (like batch norm). It's especially useful in Transformers where batch size can vary or inputs are padded.\n",
    "\n",
    "### üîπ Mental Model\n",
    "\n",
    "Imagine your input is a row of test scores across subjects. LN makes sure the average is 0 and the spread is 1 ‚Äî per student. So each row becomes normalized.\n",
    "\n",
    "### üîπ Minimal Concepts You Need\n",
    "\n",
    "* LN normalizes across **last dimension** (i.e., features)\n",
    "* BatchNorm normalizes across **batch dimension**\n",
    "* LN uses **learnable scale (Œ≥)** and **shift (Œ≤)** so the model can \"undo\" the normalization if needed\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Core Implementation (Covers Basic + Intermediate in one go)\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyLayerNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(dim))  # scale\n",
    "        self.beta = nn.Parameter(torch.zeros(dim))  # shift\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)         # mean over features\n",
    "        std = x.std(dim=-1, keepdim=True)           # std over features\n",
    "        norm_x = (x - mean) / (std + self.eps)       # normalize\n",
    "        return self.gamma * norm_x + self.beta       # scale and shift\n",
    "\n",
    "# Example usage\n",
    "x = torch.randn(3, 4)  # 3 samples, 4 features\n",
    "layer_norm = MyLayerNorm(4)\n",
    "output = layer_norm(x)\n",
    "```\n",
    "\n",
    "‚úÖ This gives you:\n",
    "\n",
    "* Manual implementation\n",
    "* Learnable parameters (Œ≥ and Œ≤)\n",
    "* Epsilon control\n",
    "* Works with any shape like (B, S, D)\n",
    "\n",
    "### üîπ üîç Comparison\n",
    "\n",
    "```python\n",
    "nn.LayerNorm(4)(x)  # Should match MyLayerNorm(4)(x)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Residual Connections ‚Äî Simplified Mastery Path\n",
    "\n",
    "### üîπ What is it?\n",
    "\n",
    "They **add the input back to the output** of a layer:\n",
    "\n",
    "```\n",
    "output = Layer(x)\n",
    "x + output ‚Üí next layer\n",
    "```\n",
    "\n",
    "### üîπ Why?\n",
    "\n",
    "* Prevent **vanishing gradients**\n",
    "* Let the network **focus on learning what‚Äôs different**\n",
    "* Help deep models train stably from the start\n",
    "\n",
    "### üîπ Mental Model\n",
    "\n",
    "You're trying to improve a sentence. Instead of rewriting from scratch, you just **add corrections** to the existing one. Residual = \"what I want to change.\"\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Core Implementation\n",
    "\n",
    "```python\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim, dim)\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.norm(self.layer(x))  # Residual + LayerNorm\n",
    "```\n",
    "\n",
    "This gives:\n",
    "\n",
    "* Residual connection\n",
    "* LayerNorm (post-normalization)\n",
    "* Handles dimension match\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Bonus: Visualization of Gradient Flow\n",
    "\n",
    "```python\n",
    "from torchviz import make_dot\n",
    "x = torch.randn(1, 4, requires_grad=True)\n",
    "model = ResidualBlock(4)\n",
    "y = model(x)\n",
    "make_dot(y, params=dict(list(model.named_parameters()) + [('x', x)]))\n",
    "```\n",
    "\n",
    "You'll see how the gradient flows through both the **main path** and the **shortcut**.\n",
    "\n",
    "---\n",
    "\n",
    "## üîÑ TL;DR Table\n",
    "\n",
    "| Concept             | BatchNorm     | LayerNorm        |\n",
    "| ------------------- | ------------- | ---------------- |\n",
    "| Normalized over     | Batch dim (0) | Feature dim (-1) |\n",
    "| Good for            | Vision (CNNs) | NLP/Transformers |\n",
    "| Œ≥, Œ≤                | Learnable     | Learnable        |\n",
    "| Affected by padding | Yes (bad)     | No (good)        |\n",
    "\n",
    "| Residual Connection                        |\n",
    "| ------------------------------------------ |\n",
    "| Adds `input + output` of a block           |\n",
    "| Helps in stable training and gradient flow |\n",
    "| Key in all Transformer layers              |\n",
    "\n",
    "---\n",
    "\n",
    "Would you like the same kind of guided path for the **Transformer Encoder block as a whole** (with all components linked)?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f337112c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d226be7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb5328dd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0cc532cd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
