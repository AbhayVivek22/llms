# The Complete Transformer Architecture: From Text to Output

## 1. Introduction

The transformer architecture, introduced in the paper "Attention Is All You Need" (Vaswani et al., 2017), revolutionized natural language processing. Unlike previous sequence models that processed text sequentially (like RNNs and LSTMs), transformers process entire sequences in parallel, using self-attention mechanisms to capture relationships between all words in a sequence.

The basic structure consists of:
- An **encoder stack** that processes the input sequence
- A **decoder stack** that generates the output sequence
- Various sub-components including self-attention, feed-forward networks, and normalization layers

Let's follow a piece of text through the entire architecture, explaining each transformation in detail.

## 2. Input Processing

### 2.1 Tokenization

The first step is breaking the input text into tokens (words or subwords).

```python
import torch
from transformers import AutoTokenizer

# Load a pre-trained tokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# Example input text
text = "The transformer architecture is powerful."

# Tokenize the input text
tokens = tokenizer(text, return_tensors="pt")
input_ids = tokens["input_ids"]  # Shape: [batch_size, sequence_length]
attention_mask = tokens["attention_mask"]  # Shape: [batch_size, sequence_length]

print("Original text:", text)
print("Tokenized ids:", input_ids)
print("Token words:", tokenizer.convert_ids_to_tokens(input_ids[0]))
```

The tokenization process:
1. Splits text into subword units (varies by tokenizer type)
2. Adds special tokens like [CLS] (start of sequence) and [SEP] (end of sequence)
3. Converts tokens to numerical IDs using a vocabulary lookup
4. Creates an attention mask (1 for real tokens, 0 for padding)

### 2.2 Token Embedding

Next, we convert token IDs into dense vector representations (embeddings).

```python
class TokenEmbedding(torch.nn.Module):
    def __init__(self, vocab_size, embedding_dim):
        super().__init__()
        self.embedding = torch.nn.Embedding(vocab_size, embedding_dim)
        self.embedding_dim = embedding_dim
        
    def forward(self, x):
        # x shape: [batch_size, seq_length]
        # output shape: [batch_size, seq_length, embedding_dim]
        return self.embedding(x) * (self.embedding_dim ** 0.5)

# Example usage
vocab_size = tokenizer.vocab_size  # e.g., 30522 for BERT
embedding_dim = 512
token_embedding_layer = TokenEmbedding(vocab_size, embedding_dim)

# Convert token IDs to embeddings
token_embeddings = token_embedding_layer(input_ids)  # Shape: [batch_size, seq_length, embedding_dim]
print(f"Token embeddings shape: {token_embeddings.shape}")
```

Each token is mapped to a dense vector of size `embedding_dim`. The multiplication by `(embedding_dim ** 0.5)` is a scaling factor that helps with training stability.

### 2.3 Positional Encoding

Since transformers process tokens in parallel rather than sequentially, we need to inject information about token positions.

```python
import torch
import torch.nn as nn
import math

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super().__init__()

        # Create a [max_len, d_model] tensor to hold positional values
        pe = torch.zeros(max_len, d_model)

        # Each row is the position (0 to max_len-1)
        positions = torch.arange(0, max_len).unsqueeze(1)  # shape: [max_len, 1]

        # This controls how fast frequencies change for each dimension
        # We use even positions for sin, odd for cos
        frequencies = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))

        # Fill even columns with sine of (position Ã— frequency)
        pe[:, 0::2] = torch.sin(positions * frequencies)

        # Fill odd columns with cosine of (position Ã— frequency)
        pe[:, 1::2] = torch.cos(positions * frequencies)

        # Shape becomes [1, max_len, d_model] so we can add to embeddings later
        pe = pe.unsqueeze(0)

        # Store this in the model (wonâ€™t be updated by gradients)
        self.register_buffer('pe', pe)

    def forward(self, x):
        # x shape: [batch_size, seq_len, d_model]
        # Add position encoding (only up to sequence length)
        return x + self.pe[:, :x.size(1)]


ðŸ§  What to Remember:
    - positions: [0, 1, 2, ..., seq_len-1] â†’ represents the position of each word.
    - frequencies: control how wavy the sine/cosine is in each dimension.
    - We alternate between sine and cosine for each dimension in d_model.
    - This gives every position a unique pattern, and the model can learn to use it.

# Example usage
positional_encoding = PositionalEncoding(embedding_dim)
embeddings_with_position = positional_encoding(token_embeddings)
print(f"Embeddings with positional encoding: {embeddings_with_position.shape}")
```

The positional encoding uses sine and cosine functions of different frequencies to encode position information. Each position has a unique pattern, and similar positions have similar patterns. The positional encodings are added directly to the token embeddings.

**Visual representation**: Imagine a matrix where each row represents a token position, and each column represents a dimension in the embedding space. The values follow sinusoidal patterns that create a unique "fingerprint" for each position.

## 3. The Encoder

The encoder consists of multiple identical layers stacked on top of each other (typically 6-12 layers). Each layer has two main components:
1. Multi-head self-attention
2. Position-wise feed-forward network

Let's build the encoder step by step.

### 3.1 Self-Attention Mechanism

The self-attention mechanism is the core innovation of transformers. It allows each token to "look at" all other tokens in the sequence and compute its representation based on relevant tokens.

#### 3.1.1 Scaled Dot-Product Attention

```python
def scaled_dot_product_attention(query, key, value, mask=None):
    """
    Compute scaled dot-product attention.
    
    Args:
        query: Query vectors [batch_size, num_heads, seq_length, head_dim]
        key: Key vectors [batch_size, num_heads, seq_length, head_dim]
        value: Value vectors [batch_size, num_heads, seq_length, head_dim]
        mask: Optional mask [batch_size, 1, 1, seq_length] or [batch_size, 1, seq_length, seq_length]
    
    Returns:
        Output vectors and attention weights
    """
    # Compute attention scores
    # (batch_size, num_heads, seq_length, head_dim) Ã— (batch_size, num_heads, head_dim, seq_length)
    # â†’ (batch_size, num_heads, seq_length, seq_length)
    matmul_qk = torch.matmul(query, key.transpose(-2, -1))
    
    # Scale attention scores
    d_k = query.size(-1)
    scaled_attention_logits = matmul_qk / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))
    
    # Apply mask (if provided)
    if mask is not None:
        # Add large negative value to masked positions to make softmax â†’ 0
        scaled_attention_logits = scaled_attention_logits.masked_fill(mask == 0, -1e9)
    
    # Apply softmax to get attention weights
    # (batch_size, num_heads, seq_length, seq_length)
    attention_weights = torch.nn.functional.softmax(scaled_attention_logits, dim=-1)
    
    # Apply attention weights to values
    # (batch_size, num_heads, seq_length, seq_length) Ã— (batch_size, num_heads, seq_length, head_dim)
    # â†’ (batch_size, num_heads, seq_length, head_dim)
    output = torch.matmul(attention_weights, value)
    
    return output, attention_weights
```

The attention mechanism:
1. Computes "compatibility" between each query and all keys via dot products
2. Scales these scores by dividing by sqrt(d_k) to prevent extremely small gradients
3. Applies a softmax to get attention weights (probabilities that sum to 1)
4. Computes a weighted sum of values based on these weights

**Visual representation**: Picture a sequence of words where each word has three vectors (Q, K, V). Each word calculates how much it should "pay attention" to every other word by comparing its Q vector with all other words' K vectors. Then it forms its new representation by taking a weighted average of all words' V vectors, where the weights are determined by the attention scores.

#### 3.1.2 Multi-Head Attention

Instead of performing a single attention function, transformers use multiple "heads" of attention, allowing the model to focus on different parts of the sequence.

```python
class MultiHeadAttention(torch.nn.Module):
    def __init__(self, d_model, num_heads):
        super().__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        self.head_dim = d_model // num_heads
        
        assert d_model % num_heads == 0, "d_model must be divisible by num_heads"
        
        # Linear projections for Q, K, V for all heads at once
        self.wq = torch.nn.Linear(d_model, d_model)
        self.wk = torch.nn.Linear(d_model, d_model)
        self.wv = torch.nn.Linear(d_model, d_model)
        
        # Output projection
        self.wo = torch.nn.Linear(d_model, d_model)
        
    def split_heads(self, x):
        """Split the last dimension into (num_heads, head_dim)."""
        batch_size, seq_length, _ = x.size()
        
        # Reshape to [batch_size, seq_length, num_heads, head_dim]
        x = x.view(batch_size, seq_length, self.num_heads, self.head_dim)
        
        # Transpose to [batch_size, num_heads, seq_length, head_dim]
        return x.transpose(1, 2)
        
    def combine_heads(self, x):
        """Combine the multiple heads back into d_model dimension."""
        batch_size, _, seq_length, _ = x.size()
        
        # Transpose back to [batch_size, seq_length, num_heads, head_dim]
        x = x.transpose(1, 2)
        
        # Combine last two dimensions: [batch_size, seq_length, d_model]
        return x.contiguous().view(batch_size, seq_length, self.d_model)
    
    def forward(self, query, key, value, mask=None):
        batch_size = query.size(0)
        
        # Linear projections
        q = self.wq(query)  # [batch_size, seq_length, d_model]
        k = self.wk(key)    # [batch_size, seq_length, d_model]
        v = self.wv(value)  # [batch_size, seq_length, d_model]
        
        # Split heads
        q = self.split_heads(q)  # [batch_size, num_heads, seq_length, head_dim]
        k = self.split_heads(k)  # [batch_size, num_heads, seq_length, head_dim]
        v = self.split_heads(v)  # [batch_size, num_heads, seq_length, head_dim]
        
        # Scaled dot-product attention
        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)
        # scaled_attention: [batch_size, num_heads, seq_length, head_dim]
        
        # Combine heads
        concat_attention = self.combine_heads(scaled_attention)  # [batch_size, seq_length, d_model]
        
        # Final linear projection
        output = self.wo(concat_attention)  # [batch_size, seq_length, d_model]
        
        return output, attention_weights

# Example usage
d_model = embedding_dim  # 512
num_heads = 8
multi_head_attention = MultiHeadAttention(d_model, num_heads)

# In self-attention, query, key, and value are all the same
mha_output, attention_weights = multi_head_attention(
    embeddings_with_position,
    embeddings_with_position,
    embeddings_with_position,
    mask=None
)
print(f"Multi-head attention output shape: {mha_output.shape}")
print(f"Attention weights shape: {attention_weights.shape}")
```

The multi-head attention process:
1. Projects input embeddings to query, key, and value vectors
2. Splits these projections into multiple heads
3. Performs scaled dot-product attention for each head independently
4. Concatenates the results and applies a final projection

**Visual representation**: Imagine 8 different "viewpoints" (attention heads) examining the same sequence, each focusing on different relationship patterns. One head might focus on subject-verb relationships, another on adjective-noun relationships, etc. These different "views" are then combined into a comprehensive representation.

### 3.2 Feed-Forward Network

After the attention layer, each position goes through an identical feed-forward network.

```python
class PositionwiseFeedForward(torch.nn.Module):
    def __init__(self, d_model, d_ff):
        super().__init__()
        self.linear1 = torch.nn.Linear(d_model, d_ff)
        self.linear2 = torch.nn.Linear(d_ff, d_model)
        self.relu = torch.nn.ReLU()
        
    def forward(self, x):
        # x shape: [batch_size, seq_length, d_model]
        x = self.linear1(x)  # [batch_size, seq_length, d_ff]
        x = self.relu(x)     # [batch_size, seq_length, d_ff]
        x = self.linear2(x)  # [batch_size, seq_length, d_model]
        return x

# Example usage
d_ff = 2048  # Dimension of feed-forward network
feed_forward = PositionwiseFeedForward(d_model, d_ff)
ff_output = feed_forward(mha_output)
print(f"Feed-forward output shape: {ff_output.shape}")
```

The feed-forward network:
1. Expands the dimension from d_model to d_ff (typically 4x larger)
2. Applies a ReLU activation
3. Projects back to the original dimension d_model

This is applied to each position independently, meaning the same network processes each token without cross-token interaction.

### 3.3 Layer Normalization

Transformers use layer normalization to stabilize training.

```python
class LayerNorm(torch.nn.Module):
    def __init__(self, features, eps=1e-6):
        super().__init__()
        self.gamma = torch.nn.Parameter(torch.ones(features))
        self.beta = torch.nn.Parameter(torch.zeros(features))
        self.eps = eps
        
    def forward(self, x):
        # x shape: [batch_size, seq_length, features]
        # Normalization along the features dimension
        mean = x.mean(dim=-1, keepdim=True)
        std = x.std(dim=-1, keepdim=True)
        return self.gamma * (x - mean) / (std + self.eps) + self.beta

# Example usage
layer_norm = LayerNorm(d_model)
normalized_output = layer_norm(ff_output)
print(f"Normalized output shape: {normalized_output.shape}")
```

Layer normalization:
1. Computes mean and standard deviation across the feature dimension
2. Normalizes each token's feature vector
3. Applies learnable scale (gamma) and shift (beta) parameters

Unlike batch normalization, layer normalization works consistently during training and inference since it normalizes across features, not across the batch.

### 3.4 Residual Connections

Each sub-layer (attention and feed-forward) is wrapped with a residual connection followed by layer normalization.

```python
class EncoderLayer(torch.nn.Module):
    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        super().__init__()
        self.mha = MultiHeadAttention(d_model, num_heads)
        self.ff = PositionwiseFeedForward(d_model, d_ff)
        self.norm1 = LayerNorm(d_model)
        self.norm2 = LayerNorm(d_model)
        self.dropout = torch.nn.Dropout(dropout)
        
    def forward(self, x, mask=None):
        # Multi-head attention with residual connection and layer norm
        attn_output, attention_weights = self.mha(x, x, x, mask)
        attn_output = self.dropout(attn_output)
        out1 = self.norm1(x + attn_output)  # Add & Norm
        
        # Feed-forward with residual connection and layer norm
        ff_output = self.ff(out1)
        ff_output = self.dropout(ff_output)
        out2 = self.norm2(out1 + ff_output)  # Add & Norm
        
        return out2, attention_weights

# Example usage
encoder_layer = EncoderLayer(d_model, num_heads, d_ff)
encoder_output, attention_weights = encoder_layer(embeddings_with_position)
print(f"Encoder layer output shape: {encoder_output.shape}")
```

The residual connections:
1. Add the input of each sub-layer to its output
2. Help with gradient flow during training
3. Allow the model to "bypass" certain layers when needed

**Visual representation**: Think of residual connections as "shortcuts" that allow information to flow directly from earlier layers to later layers, skipping the intermediate transformations if they're not useful.

### 3.5 Complete Encoder Stack

The full encoder consists of multiple identical encoder layers stacked on top of each other.

```python
class Encoder(torch.nn.Module):
    def __init__(self, d_model, num_heads, d_ff, num_layers, vocab_size, max_seq_length, dropout=0.1):
        super().__init__()
        self.embedding = TokenEmbedding(vocab_size, d_model)
        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)
        self.dropout = torch.nn.Dropout(dropout)
        
        # Stack of encoder layers
        self.encoder_layers = torch.nn.ModuleList([
            EncoderLayer(d_model, num_heads, d_ff, dropout) 
            for _ in range(num_layers)
        ])
        
    def forward(self, x, mask=None):
        # x shape: [batch_size, seq_length]
        
        # Token embedding and positional encoding
        x = self.embedding(x)  # [batch_size, seq_length, d_model]
        x = self.positional_encoding(x)  # [batch_size, seq_length, d_model]
        x = self.dropout(x)
        
        attention_weights = {}
        
        # Process through encoder layers
        for i, layer in enumerate(self.encoder_layers):
            x, attention = layer(x, mask)
            attention_weights[f'encoder_layer{i+1}'] = attention
            
        return x, attention_weights

# Example usage
num_layers = 6
max_seq_length = 512
encoder = Encoder(d_model, num_heads, d_ff, num_layers, 
                  vocab_size=tokenizer.vocab_size,
                  max_seq_length=max_seq_length)

encoder_output, all_attention_weights = encoder(input_ids)
print(f"Final encoder output shape: {encoder_output.shape}")
```

The encoder stack process:
1. Starts with token embeddings + positional encoding
2. Passes through multiple encoder layers
3. Each layer uses the output of the previous layer as input
4. The final output contains contextual representations of each input token

At this point, each token's representation has been enriched with contextual information from the entire sequence. These representations will be used by the decoder to generate the output sequence.

## 4. The Decoder

The decoder is responsible for generating the output sequence. It has a similar structure to the encoder but with additional components.

### 4.1 Decoder Structure

The decoder has three main components in each layer:
1. Masked multi-head self-attention (to prevent looking at future tokens)
2. Multi-head cross-attention (to attend to the encoder output)
3. Position-wise feed-forward network

Let's implement the decoder components:

```python
class DecoderLayer(torch.nn.Module):
    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        super().__init__()
        self.mha1 = MultiHeadAttention(d_model, num_heads)  # Masked self-attention
        self.mha2 = MultiHeadAttention(d_model, num_heads)  # Cross-attention with encoder output
        self.ff = PositionwiseFeedForward(d_model, d_ff)
        self.norm1 = LayerNorm(d_model)
        self.norm2 = LayerNorm(d_model)
        self.norm3 = LayerNorm(d_model)
        self.dropout = torch.nn.Dropout(dropout)
        
    def forward(self, x, enc_output, look_ahead_mask=None, padding_mask=None):
        # x shape: [batch_size, target_seq_length, d_model]
        # enc_output shape: [batch_size, input_seq_length, d_model]
        
        # Masked self-attention with residual connection and layer norm
        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)
        attn1 = self.dropout(attn1)
        out1 = self.norm1(x + attn1)  # Add & Norm
        
        # Cross-attention with encoder output
        attn2, attn_weights_block2 = self.mha2(
            out1, enc_output, enc_output, padding_mask)
        attn2 = self.dropout(attn2)
        out2 = self.norm2(out1 + attn2)  # Add & Norm
        
        # Feed-forward with residual connection and layer norm
        ff_output = self.ff(out2)
        ff_output = self.dropout(ff_output)
        out3 = self.norm3(out2 + ff_output)  # Add & Norm
        
        return out3, attn_weights_block1, attn_weights_block2

# Example usage: Create the look-ahead mask for the decoder
def create_look_ahead_mask(size):
    """
    Create a mask to prevent the decoder from looking at future tokens.
    The upper triangular part (including the diagonal) is set to 1.
    """
    mask = torch.triu(torch.ones((size, size)), diagonal=1).type(torch.bool)
    return mask  # shape: [size, size]

# Example
target_seq_length = 5
look_ahead_mask = create_look_ahead_mask(target_seq_length)
print("Look-ahead mask:")
print(look_ahead_mask)

# Create example decoder input
decoder_input_ids = torch.tensor([[101, 2061, 3319, 2003, 102]])  # Example: [CLS] This model is [SEP]

# Instantiate the decoder layer
decoder_layer = DecoderLayer(d_model, num_heads, d_ff)

# Create embeddings for decoder input
decoder_embedding = TokenEmbedding(tokenizer.vocab_size, d_model)
decoder_pos_encoding = PositionalEncoding(d_model, max_seq_length)
decoder_input = decoder_embedding(decoder_input_ids)
decoder_input_with_pos = decoder_pos_encoding(decoder_input)

# Pass through decoder layer
decoder_output, self_attn, cross_attn = decoder_layer(
    decoder_input_with_pos, 
    encoder_output,
    look_ahead_mask=look_ahead_mask
)
print(f"Decoder layer output shape: {decoder_output.shape}")
```

The key differences in the decoder:
1. **Masked self-attention**: Uses a triangular mask to prevent attending to future positions
2. **Cross-attention**: Attends to the encoder output, allowing the decoder to focus on relevant parts of the input sequence
3. **Sequential generation**: During inference, tokens are generated one at a time, with the decoder attending to previously generated tokens

**Visual representation**: In the decoder, each token can only look at tokens that come before it (masked self-attention), plus all encoder outputs (cross-attention). This creates a unidirectional flow of information in the decoder, while still allowing it to draw information from the entire input sequence.

### 4.2 Complete Decoder Stack

Similar to the encoder, the decoder consists of multiple identical layers stacked on top of each other.

```python
class Decoder(torch.nn.Module):
    def __init__(self, d_model, num_heads, d_ff, num_layers, vocab_size, max_seq_length, dropout=0.1):
        super().__init__()
        self.embedding = TokenEmbedding(vocab_size, d_model)
        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)
        self.dropout = torch.nn.Dropout(dropout)
        
        # Stack of decoder layers
        self.decoder_layers = torch.nn.ModuleList([
            DecoderLayer(d_model, num_heads, d_ff, dropout) 
            for _ in range(num_layers)
        ])
        
    def forward(self, x, enc_output, look_ahead_mask=None, padding_mask=None):
        # x shape: [batch_size, target_seq_length]
        
        # Token embedding and positional encoding
        x = self.embedding(x)  # [batch_size, target_seq_length, d_model]
        x = self.positional_encoding(x)  # [batch_size, target_seq_length, d_model]
        x = self.dropout(x)
        
        attention_weights = {}
        
        # Process through decoder layers
        for i, layer in enumerate(self.decoder_layers):
            x, self_attn, cross_attn = layer(x, enc_output, look_ahead_mask, padding_mask)
            
            attention_weights[f'decoder_layer{i+1}_self_attn'] = self_attn
            attention_weights[f'decoder_layer{i+1}_cross_attn'] = cross_attn
            
        return x, attention_weights

# Example usage
decoder = Decoder(d_model, num_heads, d_ff, num_layers, 
                 vocab_size=tokenizer.vocab_size,
                 max_seq_length=max_seq_length)

decoder_output, decoder_attention_weights = decoder(
    decoder_input_ids, 
    encoder_output,
    look_ahead_mask=look_ahead_mask
)
print(f"Final decoder output shape: {decoder_output.shape}")
```

The decoder stack process:
1. Starts with token embeddings + positional encoding for the target sequence
2. Each decoder layer attends to previously generated tokens (masked self-attention)
3. Each decoder layer also attends to the encoder output (cross-attention)
4. Each token's representation is enriched with information from the input sequence and previous output tokens

### 4.3 Output Processing

The final step is to convert the decoder output into probabilities over the vocabulary.

```python
class Transformer(torch.nn.Module):
    def __init__(self, d_model, num_heads, d_ff, num_layers, input_vocab_size, 
                 target_vocab_size, max_seq_length, dropout=0.1):
        super().__init__()
        
        self.encoder = Encoder(
            d_model, num_heads, d_ff, num_layers, 
            input_vocab_size, max_seq_length, dropout
        )
        
        self.decoder = Decoder(
            d_model, num_heads, d_ff, num_layers, 
            target_vocab_size, max_seq_length, dropout
        )
        
        # Final linear layer to project to vocabulary size
        self.final_layer = torch.nn.Linear(d_model, target_vocab_size)
        
    def forward(self, input_ids, target_ids, enc_padding_mask=None, 
               look_ahead_mask=None, dec_padding_mask=None):
        # input_ids shape: [batch_size, input_seq_length]
        # target_ids shape: [batch_size, target_seq_length]
        
        # Encoding
        enc_output, enc_attention_weights = self.encoder(input_ids, enc_padding_mask)
        # enc_output shape: [batch_size, input_seq_length, d_model]
        
        # Decoding
        dec_output, dec_attention_weights = self.decoder(
            target_ids, enc_output, look_ahead_mask, dec_padding_mask)
        # dec_output shape: [batch_size, target_seq_length, d_model]
        
        # Final linear projection
        logits = self.final_layer(dec_output)  # [batch_size, target_seq_length, target_vocab_size]
        
        return logits, enc_attention_weights, dec_attention_weights

# Example usage
transformer = Transformer(
    d_model=d_model,
    num_heads=num_heads,
    d_ff=d_ff,
    num_layers=num_layers,
    input_vocab_size=tokenizer.vocab_size,
    target_vocab_size=tokenizer.vocab_size,
    max_seq_length=max_seq_length
)

# Create look-ahead mask for the decoder
target_seq_length = decoder_input_ids.size(1)
look_ahead_mask = create_look_ahead_mask(target_seq_length)

# Forward pass
logits, enc_attention, dec_attention = transformer(
    input_ids, 
    decoder_input_ids,
    look_ahead_mask=look_ahead_mask
)

print(f"Output logits shape: {logits.shape}")  # [batch_size, target_seq_length, vocab_size]
```

The output processing:
1. The decoder output is projected to the vocabulary size using a linear layer
2. This gives us unnormalized logits for each position
3. These logits can be converted to probabilities using softmax
4. During training, we compute the cross-entropy loss between these predictions and the true target tokens
5. During inference, we typically use beam search or greedy decoding to generate the most likely sequence

## 5. Training Process

The transformer is typically trained using teacher forcing, where the true target sequence is provided as input to the decoder during training.

```python
def train_transformer(transformer, optimizer, input_ids, target_ids, target_mask):
    """
    Train the transformer for one step.
    
    Args:
        transformer: The transformer model
        optimizer: The optimizer
        input_ids: Input token IDs [batch_size, input_seq_length]
        target_ids: Target token IDs [batch_size, target_seq_length]
        target_mask: Mask for the target sequence [batch_size, target_seq_length]
    
    Returns:
        Loss value
    """
    # Create look-ahead mask for the decoder
    target_seq_length = target_ids.size(1)
    look_ahead_mask = create_look_ahead_mask(target_seq_length)
    
    # Forward pass
    logits, _, _ = transformer(
        input_ids, 
        target_ids[:, :-1],  # Remove the last token from target input
        look_ahead_mask=look_ahead_mask
    )
    
    # Compute loss
    loss_fn = torch.nn.CrossEntropyLoss(ignore_index=0)  # 0 is padding token
    loss = loss_fn(
        logits.reshape(-1, logits.size(-1)),  # [batch_size * target_seq_length, vocab_size]
        target_ids[:, 1:].reshape(-1)  # [batch_size * target_seq_length]
    )
    
    # Backward pass
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    return loss.item()

# Example usage
optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)

# Dummy target mask (1 for real tokens, 0 for padding)
target_mask = torch.ones_like(decoder_input_ids)

# Train for one step
loss = train_transformer(transformer, optimizer, input_ids, decoder_input_ids, target_mask)
print(f"Training loss: {loss}")
```

The training process:
1. Input sequence is encoded by the encoder
2. Decoder receives the true target sequence (shifted right) and encoder output
3. Masked self-attention ensures the decoder can only look at previous tokens
4. The model predicts the next token for each position
5. Loss is computed between predictions and true target tokens
6. Backpropagation updates the model parameters

**Visual representation**: During training, the decoder is given the correct tokens at each step (except the last one), and it learns to predict the next token. This is like having a "cheat sheet" that gradually gets covered up as you move through the sequence.

## 6. Inference Process

During inference, we generate tokens sequentially, feeding each generated token back into the decoder.

```python
def generate_sequence(transformer, input_ids, max_length=50, start_token_id=101, end_token_id=102):
    """
    Generate a sequence using the transformer model.
    
    Args:
        transformer: The transformer model
        input_ids: Input token IDs [batch_size, input_seq_length]
        max_length: Maximum output sequence length
        start_token_id: ID of the start token
        end_token_id: ID of the end token
    
    Returns:
        Generated sequence
    """
    # Encode the input sequence
    enc_output, _ = transformer.encoder(input_ids)
    
    # Initialize decoder input with start token
    batch_size = input_ids.size(0)
    decoder_input = torch.ones((batch_size, 1), dtype=torch.long) * start_token_id
    
    # Generate tokens sequentially
    for i in range(max_length - 1):
        # Create look-ahead mask
        look_ahead_mask = create_look_ahead_mask(decoder_input.size(1))
        
        # Get decoder output
        dec_output, _ = transformer.decoder(decoder_input, enc_output, look_ahead_mask)
        
        # Get prediction for the last token
        logits = transformer.final_layer(dec_output[:, -1:, :])  # [batch_size, 1, vocab_size]
        
        # Get the most likely token
        predicted_id = torch.argmax(logits, dim=-1)  # [batch_size, 1]
        
        # Concatenate to decoder input
        decoder_input = torch.cat([decoder_input, predicted_id], dim=1)
        
        # Check if end token is generated
        if predicted_id[0, 0].item() == end_token_id:
            break
    
    return decoder_input

# Example usage
generated_ids = generate_sequence(transformer, input_ids)
generated_text = tokenizer.decode(generated_ids[0])
print(f"Generated text: {generated_text}")
```

The inference process:
1. Input sequence is encoded by the encoder
2. Decoder starts with a start token
3. Generated tokens are fed back into the decoder one by one
4. Process continues until an end token is generated or max length is reached

**Visual representation**: During inference, the decoder is like a writer generating one word at a time, constantly referring back to the entire input (through cross-attention) and all previously written words (through masked self-attention). After writing each word, it becomes part of the context for generating the next word.

## 7. Beam Search

In practice, greedy decoding (picking the most likely token at each step) often yields suboptimal results. Beam search maintains multiple hypotheses, improving output quality.

```python
def beam_search(transformer, input_ids, beam_size=4, max_length=50, start_token_id=101, end_token_id=102):
    """
    Generate a sequence using beam search.
    
    Args:
        transformer: The transformer model
        input_ids: Input token IDs [batch_size, input_seq_length]
        beam_size: Number of beams to maintain
        max_length: Maximum output sequence length
        start_token_id: ID of the start token
        end_token_id: ID of the end token
    
    Returns:
        Best generated sequence
    """
    # Encode the input sequence
    enc_output, _ = transformer.encoder(input_ids)
    
    # Initialize beam with start token
    batch_size = input_ids.size(0)
    decoder_input = torch.ones((batch_size, 1), dtype=torch.long) * start_token_id
    
    # Initialize beam score
    beam_scores = torch.zeros((batch_size, beam_size))
    
    # Initialize beams
    beams = [decoder_input] * beam_size
    
    # Generate tokens sequentially
    for i in range(max_length - 1):
        next_beams = []
        next_beam_scores = []
        
        # Expand each current beam
        for beam_idx, beam in enumerate(beams):
            # Create look-ahead mask
            look_ahead_mask = create_look_ahead_mask(beam.size(1))
            
            # Get decoder output
            dec_output, _ = transformer.decoder(beam, enc_output, look_ahead_mask)
            
            # Get prediction for the last token
            logits = transformer.final_layer(dec_output[:, -1:, :])  # [batch_size, 1, vocab_size]
            
            # Get probabilities with softmax
            probs = torch.nn.functional.softmax(logits, dim=-1)  # [batch_size, 1, vocab_size]
            
            # Get top-k probabilities and indices
            topk_probs, topk_indices = torch.topk(probs, beam_size, dim=-1)
            
            # Update beam scores
            current_score = beam_scores[:, beam_idx].unsqueeze(1).unsqueeze(2)
            new_scores = current_score + torch.log(topk_probs)
            next_beam_scores.append(new_scores.squeeze(1))
            
            # Create new beams
            for k in range(beam_size):
                new_beam = torch.cat([beam, topk_indices[:, :, k]], dim=1)
                next_beams.append(new_beam)
        
        # Flatten and find top-k beams
        next_beam_scores = torch.cat(next_beam_scores, dim=1)  # [batch_size, beam_size * beam_size]
        top_beam_scores, top_beam_indices = torch.topk(next_beam_scores, beam_size, dim=1)
        
        # Select top-k beams
        new_beams = []
        for batch_idx in range(batch_size):
            for beam_idx in range(beam_size):
                idx = top_beam_indices[batch_idx, beam_idx].item()
                new_beams.append(next_beams[idx])
        
        beams = new_beams
        beam_scores = top_beam_scores
        
        # Check if all beams end with end token
        all_ended = True
        for beam in beams:
            if beam[0, -1].item() != end_token_id:
                all_ended = False
                break
        
        if all_ended:
            break
    
    # Return the beam with highest score
    best_beam_idx = torch.argmax(beam_scores, dim=1)[0].item()
    return beams[best_beam_idx]

# Example usage
generated_ids = beam_search(transformer, input_ids)
generated_text = tokenizer.decode(generated_ids[0])
print(f"Generated text (beam search): {generated_text}")
```

Beam search:
1. Maintains multiple candidate sequences (beams)
2. At each step, expands each beam with potential next tokens
3. Keeps the top-k sequences based on cumulative probability
4. Results in more fluent and coherent output than greedy decoding

**Visual representation**: Beam search is like considering multiple possible continuations at each step. Imagine writing a story and exploring several different ways the next sentence could go, then keeping only the most promising versions to continue developing.

## 8. Practical Implementation: Complete PyTorch Model

Here's how you would implement and use a complete transformer model with PyTorch:

```python
import torch
import torch.nn as nn
import math
from transformers import AutoTokenizer

class Transformer(nn.Module):
    """Complete transformer model for sequence-to-sequence tasks."""
    
    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, num_heads=8, 
                 num_layers=6, d_ff=2048, max_seq_length=5000, dropout=0.1):
        super().__init__()
        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)
        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)
        self.positional_encoding = self.create_positional_encoding(max_seq_length, d_model)
        
        self.encoder_layers = nn.ModuleList([
            EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)
        ])
        
        self.decoder_layers = nn.ModuleList([
            DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)
        ])
        
        self.final_layer = nn.Linear(d_model, tgt_vocab_size)
        self.dropout = nn.Dropout(dropout)
        self.d_model = d_model
        
    def create_positional_encoding(self, max_seq_length, d_model):
        # Create positional encoding
        pe = torch.zeros(max_seq_length, d_model)
        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        
        # Apply sine to even indices, cosine to odd indices
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        
        # Add batch dimension
        pe = pe.unsqueeze(0)
        return pe
    
    def forward(self, src, tgt, src_mask=None, tgt_mask=None, memory_mask=None):
        # src shape: [batch_size, src_seq_length]
        # tgt shape: [batch_size, tgt_seq_length]
        
        # Embed and add positional encoding - Encoder
        src = self.encoder_embedding(src) * math.sqrt(self.d_model)
        src = src + self.positional_encoding[:, :src.size(1), :].to(src.device)
        src = self.dropout(src)
        
        # Embed and add positional encoding - Decoder
        tgt = self.decoder_embedding(tgt) * math.sqrt(self.d_model)
        tgt = tgt + self.positional_encoding[:, :tgt.size(1), :].to(tgt.device)
        tgt = self.dropout(tgt)
        
        # Encoder
        enc_output = src
        for enc_layer in self.encoder_layers:
            enc_output = enc_layer(enc_output, src_mask)
        
        # Decoder
        dec_output = tgt
        for dec_layer in self.decoder_layers:
            dec_output = dec_layer(dec_output, enc_output, tgt_mask, memory_mask)
        
        # Final linear projection
        output = self.final_layer(dec_output)
        
        return output

class EncoderLayer(nn.Module):
    """Encoder layer with self-attention and feed-forward network."""
    
    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        super().__init__()
        self.self_attn = MultiHeadAttention(d_model, num_heads)
        self.feed_forward = PositionwiseFeedForward(d_model, d_ff)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x, mask=None):
        # Self-attention with residual connection and layer norm
        attn_output = self.self_attn(x, x, x, mask)
        x = x + self.dropout(attn_output)
        x = self.norm1(x)
        
        # Feed-forward with residual connection and layer norm
        ff_output = self.feed_forward(x)
        x = x + self.dropout(ff_output)
        x = self.norm2(x)
        
        return x

class DecoderLayer(nn.Module):
    """Decoder layer with masked self-attention, cross-attention, and feed-forward network."""
    
    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        super().__init__()
        self.self_attn = MultiHeadAttention(d_model, num_heads)
        self.cross_attn = MultiHeadAttention(d_model, num_heads)
        self.feed_forward = PositionwiseFeedForward(d_model, d_ff)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.norm3 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x, memory, tgt_mask=None, memory_mask=None):
        # Self-attention with residual connection and layer norm
        attn_output = self.self_attn(x, x, x, tgt_mask)
        x = x + self.dropout(attn_output)
        x = self.norm1(x)
        
        # Cross-attention with residual connection and layer norm
        attn_output = self.cross_attn(x, memory, memory, memory_mask)
        x = x + self.dropout(attn_output)
        x = self.norm2(x)
        
        # Feed-forward with residual connection and layer norm
        ff_output = self.feed_forward(x)
        x = x + self.dropout(ff_output)
        x = self.norm3(x)
        
        return x

class MultiHeadAttention(nn.Module):
    """Multi-head attention with linear projections."""
    
    def __init__(self, d_model, num_heads):
        super().__init__()
        assert d_model % num_heads == 0, "d_model must be divisible by num_heads"
        
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        
        # Linear projections
        self.q_linear = nn.Linear(d_model, d_model)
        self.k_linear = nn.Linear(d_model, d_model)
        self.v_linear = nn.Linear(d_model, d_model)
        self.out_linear = nn.Linear(d_model, d_model)
        
    def forward(self, q, k, v, mask=None):
        batch_size = q.size(0)
        
        # Linear projections and reshape
        q = self.q_linear(q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        k = self.k_linear(k).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        v = self.v_linear(v).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        
        # Scaled dot-product attention
        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)
        
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        
        attention_weights = torch.nn.functional.softmax(scores, dim=-1)
        attention_output = torch.matmul(attention_weights, v)
        
        # Reshape and final linear projection
        attention_output = attention_output.transpose(1, 2).contiguous().view(
            batch_size, -1, self.d_model)
        output = self.out_linear(attention_output)
        
        return output

class PositionwiseFeedForward(nn.Module):
    """Position-wise feed-forward network."""
    
    def __init__(self, d_model, d_ff):
        super().__init__()
        self.fc1 = nn.Linear(d_model, d_ff)
        self.fc2 = nn.Linear(d_ff, d_model)
        self.relu = nn.ReLU()
        
    def forward(self, x):
        return self.fc2(self.relu(self.fc1(x)))

# Helper functions for generating masks
def create_padding_mask(seq):
    # Create mask for padding tokens (0)
    return (seq == 0).unsqueeze(1).unsqueeze(2)

def create_look_ahead_mask(size):
    # Create mask to prevent looking ahead in the decoder
    mask = torch.triu(torch.ones((size, size)), diagonal=1).bool()
    return mask

# Example of how to use the model
def transformer_example():
    # Load tokenizer and create sample data
    tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
    src_text = "Hello, how are you doing today?"
    tgt_text = "I am doing fine, thank you."
    
    # Tokenize text
    src_tokens = tokenizer(src_text, return_tensors="pt")["input_ids"]
    tgt_tokens = tokenizer(tgt_text, return_tensors="pt")["input_ids"]
    
    # Create model
    model = Transformer(
        src_vocab_size=tokenizer.vocab_size,
        tgt_vocab_size=tokenizer.vocab_size,
        d_model=512,
        num_heads=8,
        num_layers=6,
        d_ff=2048
    )
    
    # Create masks
    src_padding_mask = create_padding_mask(src_tokens)
    tgt_padding_mask = create_padding_mask(tgt_tokens)
    tgt_mask = create_look_ahead_mask(tgt_tokens.size(1))
    
    # Forward pass
    output = model(src_tokens, tgt_tokens[:, :-1], src_padding_mask, tgt_mask)
    
    # Get predictions
    predictions = torch.argmax(output, dim=-1)
    
    # Decode predictions
    predicted_text = tokenizer.decode(predictions[0])
    print(f"Source: {src_text}")
    print(f"Target: {tgt_text}")
    print(f"Predicted: {predicted_text}")
    
    return model, output, predictions

# Run example
model, output, predictions = transformer_example()
```

## 9. Data Flow Visualization

Here's a summary of how data flows through the transformer architecture:

1. **Input text â†’ Tokenization â†’ Token IDs**
   - Text is split into tokens (words/subwords)
   - Each token is assigned a numerical ID

2. **Token IDs â†’ Token Embeddings â†’ Positional Encoding â†’ Input Representation**
   - Token IDs are converted to dense vector representations
   - Positional information is added to these embeddings
   - Result: Each token has a vector that represents both its meaning and position

3. **Input Representation â†’ Encoder Stack**
   - Through multiple encoder layers:
     - Self-attention: Each token attends to all other tokens
     - Feed-forward: Each token is processed independently
     - Layer normalization and residual connections throughout
   - Result: Contextual representations for each input token

4. **Decoder Input + Encoder Output â†’ Decoder Stack**
   - Through multiple decoder layers:
     - Masked self-attention: Each token attends to previous tokens
     - Cross-attention: Each token attends to all encoder outputs
     - Feed-forward: Each token is processed independently
     - Layer normalization and residual connections throughout
   - Result: Contextual representations for each output token

5. **Decoder Output â†’ Linear Layer â†’ Softmax â†’ Output Probabilities**
   - Decoder outputs are projected to vocabulary size
   - Softmax converts logits to probabilities
   - Result: Probability distribution over vocabulary for each position

6. **Output Probabilities â†’ Token Selection â†’ Output Text**
   - Tokens are selected based on probabilities (greedy, beam search, etc.)
   - Selected tokens are converted back to text
   - Result: Generated output sequence

## 10. Conclusion

The transformer architecture's success comes from:
- Parallel processing of sequences (efficiency)
- Self-attention mechanism (capturing long-range dependencies)
- Multi-head attention (capturing different types of relationships)
- Residual connections and layer normalization (training stability)

This architecture has become the foundation for most state-of-the-art NLP models, including BERT, GPT, T5, and others. Understanding the transformer is key to understanding modern deep learning approaches to language processing.

The beauty of the transformer lies in its elegant design: by replacing recurrence with attention, it allows for more parallelization and better captures long-range dependencies in sequences, leading to significant improvements in both quality and efficiency of language models.